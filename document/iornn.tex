\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}

%\documentclass[10pt]{article}
%\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{graphicx}
%\usepackage{natbib}

\title{Inside Outside Recursive Neural Network: A Unified Framework for 
Compositional Semantics and Meaning in Context}
\author{Phong Le, Remko Scha and Willem Zuidema}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{section introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{section introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Questions}
\label{section question}

Firstly, we ask ourselves ``Which evidence is strong enough to be used for 
learning compositional semantics?'' To answer this question, we rely on the 
following observation: a human being can guess the meaning of an unknown 
word by making use of the meaning of its context. In other words, he computes 
(in his brain) the meaning of the context and then use it to predict the meaning 
of the target unknown word (by setting some contrains to narrow down a list of 
possible meanings). Hence, if he correctly predicts the meaning of the unknown word, 
we can, at some level of belief, say that he coherends the meaning of the context. 
This idea is then encaptured in the following hypothesis
\begin{quote}
\textit{Hypothesis 1:} The agreement betwen words and contexts provides evidence for unsupervised 
compositional semantics learning. 
\end{quote}

Then, there are three questions need to be answered in order to implement the 
hypothesis
\begin{enumerate}
	\item How to construct inner meaning (i.e., phrase representations)?
	\item How to construct outer meaning (i.e., context representations)?
	\item How to use inner meaning and outer meaning at the lexicon level to learn 
	compositionality functions? In other words, how to use the agreement betwen words and 
	their contexts to learn compositionality functions?
\end{enumerate}
In the next section, we answer these questions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inside Outside Recursive Neural Network}
\label{section iornn}

In this section, we answer the three questions raising in Section~\ref{section introduction}.

\subsection{Recursive Neural Network (RNN)}
\label{subsection rnn}
\cite{socher_learning_2010} answer the first question ``How to construct phrasal semantics?'' 
by Recusive Neural Network (RNN) architecture, 
which is used to compute continuous phrase representations. In order to see how RNN works, 
let's conside the following example. Assuming that there is a consitutent with parse
tree $(p_2 \; (p_1 \; x \; y) \; z)$ as in Figure~\ref{figure rnn}. We will use a neural network, 
which contains a weight matrix $\mathbf{W}_1$ for left children and a weight matrix $\mathbf{W}_2$ 
for right children, to compute parents in a bottom up manner. Firstly, we use this network 
to compute $p_1$ based on its children $x$ and $y$
\begin{equation}
	\mathbf{p}_1 = f(\mathbf{W}_1 \mathbf{x} + \mathbf{W}_2 \mathbf{y} + \mathbf{b})
\end{equation}
where $\mathbf{b}$ is a bias, $f$ is an activation function (e.g. \textit{tanh} or \textit{logistic}).
Then, we use the same network to compute $p_2$ based on its children $p_1$ and $z$
\begin{equation}
	\mathbf{p}_2 = f(\mathbf{W}_1 \mathbf{p}_1 + \mathbf{W}_2 \mathbf{z} + \mathbf{b})
\end{equation}
This process is continued until we reach the root node.  This network is trained by 
a gradient-based optimization method (e.g., gradient descent) where the gradient 
over parameters is efficiently computed thanks to the backpropagation through structure
\cite{goller_learning_1996}. Using this architecture 
(and its extensions), Socher and colleagues succesfully reach 
state-of-the-art results in syntactic parsing \cite{socher2013parsing} and 
sentiment analysis \cite{socher2013recursive}. 
\begin{figure}[h!]
	\center
	\includegraphics[scale=0.5]{RNN.png}
	\caption{Recursive Neural Network (RNN).}
	\label{figure rnn}
\end{figure}

In order to use this architecture in an unsupervised learning manner, 
\cite{socher2011semi} replace neural network in RNN by autoencoder 
(and hence the new architecture is called Recursive Autoencoder - RAE), 
which is a feedforward neural network trained by forcing output equal to 
input (see Figure~\ref{figure rae}). Training a RAE is therefore to minmize 
the sum of reconstruction errors 
(i.e., $\|[\mathbf{x}';\mathbf{y}'] - [\mathbf{x};\mathbf{y}]\|^2$) at all internall nodes. 

\begin{figure}[h!]
	\center
	\includegraphics[scale=0.5]{RAE.png}
	\caption{Recursive Autoencoder (RAE).}
	\label{figure rae}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{IORNN}
\label{subsection nlm}

None of the above architectures, RAE or RNN, compute contextual semantics. 
However, they give us a hint to do that. In this section, we will answer the second 
question ``How to construct contextual semantics?'' by a new neural network 
architecture, namely Inside Outside Recursive Neural Network (IORNN). We also 
present this architecture by using an example of a constituent and parse tree 
$(p_2 \; (p_1 \; x \; y) \; z)$ (see Figure~\ref{figure iornn}).

\begin{figure}[h!]
	\center
	\includegraphics[scale=0.5]{IORNN.png}
	\caption{Inside-Outside Recursive Neural Network (IORNN). 
	Black rectangles correspond to inner meanings, 
	white rectangles correspond to outer meanings.}
	\label{figure iornn}
\end{figure}


Each node $u$ is assigned two vectors $\mathbf{o}_u$ and $\mathbf{i}_u$. The first one,
called \textit{outer meaning}, denotes the meaning of the context; the second one, 
called \textit{inner meaning}, denotes the meaning of the phrase that the node covers.

\paragraph{Word embeddings} (e.g., $\mathbf{i}_x$)
Similar to \cite{socher_learning_2010}, and \cite{collobert_natural_2011}, given a string of binary
representations of words $(a, b, ..., w)$ (i.e., all of the entries of $w$ are zero except the one 
corresponding to the index of the word in the dictionary), 
we first compute a string of vectors $(\mathbf{i}_{a},...,\mathbf{i}_{w})$ 
representing inner meanings of those words by using 
a look-up table (i.e., word embeddings) $\mathbf{L} \in \mathbb{R}^{n \times |V|}$, 
where $|V|$ is the size of the vocabulary and $n$ is the dimensionality of the vectors. 
This look-up table $\mathbf{L}$ could be seen as a storage of lexical semantics where each column 
is a vector representation of a word. Hence, 
\begin{equation}
    \label{equation compute word vector}
    \mathbf{i}_{w} = \mathbf{L} w \in \mathbb{R}^n
\end{equation}

\paragraph{Computing inner meaning} The inner meaning of a non-terminal node, say $p_1$, is given by
\begin{equation}
	\mathbf{i}_{p_1} = f(\mathbf{W}_1^i \mathbf{i}_{x} + \mathbf{W}_2^i \mathbf{i}_{y} + \mathbf{b}^i)
	\label{equation inner}
\end{equation}
where $\mathbf{W}_1^i, \mathbf{W}_2^i$ are $n \times n$ real matrices, 
$\mathbf{b}^i$ is a bias vector, and $f(.)$ is an activation function, e.g. $tanh$ 
function. Intuitively, the inner meaning of a parent node is the function of the inner meanings 
of its children. This is similar to what \cite{socher_learning_2010} call recursive neural network.

\paragraph{Computing outer meaning} The outer meaning of the root node, $\mathbf{o}_{root}$, is initially 
set randomly, and then learnt later. To a node which is not the root, say $p_1$, the outer meaning is given by
\begin{equation}
	\mathbf{o}_{p_1} = g(\mathbf{W}_1^o \mathbf{o}_{p_2} + \mathbf{W}_2^o \mathbf{i}_{z} + \mathbf{b}^o)
	\label{equation outer}
\end{equation}
where $\mathbf{W}_1^o, \mathbf{W}_2^o$ are $n \times n$ real matrices, 
$\mathbf{b}^o$ is a bias vector, and $g(.)$ is an activation function, e.g. $tanh$ 
function. Informally speaking, the outer meaning of a node (i.e., the meaning of 
its context) is the function of the outer meaning of its parent and the inner meaning 
of its sister. 

The reader, if familiar with syntactic parsing, could recognizes the similarity between 
Equation~\ref{equation inner}, ~\ref{equation outer}
and the inside, outside probabilities given a parse tree.
Therefore, we name the architecture Inside-Outside Recursive Neural Network.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Training IORNN}
\label{subsection train iornn}

This section is to answer the final question ``How to use contextual semantics and 
lexical semantics to learn compositionality functions?''. 

According to Hypothesis 1, there must be a strong correlation 
between $\mathbf{o}_{w}$ and $\mathbf{i}_{w}$ where $w$ is any word in 
a given sentence. The simplest way to train the network is to force 
$\mathbf{o}_{w_j} = \mathbf{i}_{w_j}$; hence, learning is to minimize the following 
loss function
\begin{equation}
	J(\theta) = \sum_{s \in D} \sum_{w \in s} \| \mathbf{o}_{w} - \mathbf{i}_{w} \|
\end{equation}
where $D$ is a set of training sentences and $\theta$ are the network parameters. 
However, that could be problematic because 
the meaning of context is not necessary the meaning of the target word.

Here, based on the observation that the meaning of context sets contraints on 
selecting a word to fill in the blank, one could suggest put a \textit{softmax} neuron 
unit on the top of each $\mathbf{o}_w$ in order to compute the probability $P(x|\mathbf{o}_w)$. 
Unfortunately, as pointed out by \cite{collobert_natural_2011}, it might not work. 

Using the same method proposed by \cite{collobert_natural_2011}, we train the network 
such that it gives a higher score to the correct target word rather than to incorrect ones. 
The score $s(x,\mathbf{o}_w)$ given to a candidate word $x$ for a specific context 
$\mathbf{o}_w$ is computed by 
\begin{align}
	u(x,\mathbf{o}_w) & = f(\mathbf{W}_u^o \mathbf{o}_w + \mathbf{W}_u^i \mathbf{i}_x + \mathbf{b}_u) \\
	s(x,\mathbf{o}_w) & = \mathbf{W}_s u(x,\mathbf{o}_w) + \mathbf{b}_s
\end{align}
where $\mathbf{W}_u^o,\mathbf{W}_u^i$ are $n \times k$ real matrices, $ \mathbf{W}_s$ is 
a $k \times 1$ matrix, and $\mathbf{b}_u, \mathbf{b}_s$ are bias vectors. (We fix $k=2n$.) 
Now, the objective function is the ranking criterion with respect to $\theta$
\begin{equation}
	J(\theta) = \sum_{s \in D} \sum_{w \in s} \sum_{x \in V} \max \{0, 1 - s(w,\mathbf{o}_w) + s(x,\mathbf{o}_w) \}
\end{equation}

To minimize the above objective function, we randomly pick up a word 
in the dictionary as a corrupt example, compute the gradient, and update 
the parameters by a gradient descent method. Thanks to the backpropagation 
through structure \cite{goller_learning_1996}, the gradient is efficiently computed. 
Following \cite{socher2013recursive}, we use AdaGrad \cite{duchi2011adaptive}
to update the parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{section experiments}

In order to examine how IORNN performs on both compositional semantics learning and 
meaning in context, we evaluate it on two tasks: phrase similarity and 
word meaning in context. Because, to our knowledge, there are no frameworks 
that tackle both problems, we use vector addition and pair-wise multiplication 
as our baselines. Although these methods are simple, choose them as baselines 
are reasonable since (1) \cite{blacoe_comparison_2012} show that they perform better than RAE in the 
phrase similarity task, (2) they are widely used in many applications requiring 
compositional semantics [cite...], and (3) vector addition is widely used as a method 
to compute contextual meaning vector \cite{thater2011word}

In the all experiments, we implemented IORNN in Torch-lua \cite{collobert_implementing_2012}.
We initialled the network with the 50-dim word embeddings\footnote{\url{http://ronan.collobert.com/senna/}} 
from \cite{collobert_natural_2011}. Then we trained it on a dataset containing 1.5M sentences
from the BNC corpus (about one fourth of the whole corpus), which were parsed by 
the Berkeley parser \cite{petrov2006learning}. 
	
\subsection{Phrase Similarity}
\label{subsection phrase similarity}

Phrase similarity is the task in which one is asked to compute the meanings of (short) phrases 
and measure their semantic similarities. Its goodness is measured by comparing its judgements with 
human judgements. In this experiment, we used the dataset\footnote{\url{http://homepages.inf.ed.ac.uk/s0453356/share}} from 
\cite{mitchell_composition_2010} which contains 5832 human judgements on semantic similarity 
for noun-noun, verb-object, and adjective-noun phrases. There are 108 items; each contains a phrase pair
and human ratings from 1 (very low similarity) to 7 (very high similarity) (see Table~\ref{table compounds}).

In this task, we use the cosine distance to measure the semantic similarity, 
i.e. $d(a,b) = \cos(\mathbf{i}_a,\mathbf{i}_b)$. Following \cite{blacoe_comparison_2012}, 
\cite{hermann2013role} and many others, we compute Spearman correlation coefficient $\rho$
between model scores and human judgements.


\begin{table}[h!]
	\center
	\begin{tabular}{ccccc}
		type & phrase 1 & phrase 2 & rating \\ \hline 
		v-obj & remember name &  pass time & 3 \\ 
		adj-n & dark eye & left arm & 5 \\ 
		n-n & county council & town hall & 4 \\ \hline
	\end{tabular}
	\caption{Items in the dataset from \cite{mitchell_composition_2010}.}
	\label{table compounds}
\end{table}	

First of all, we focus on the results reported by \cite{blacoe_comparison_2012}. Blacoe \& Lapata 
claims that RAE performs worse than addition and pair-wise multiplication in all of their three settings. 
Here, we used their third setting, and, to be fair, we trained IORNN with their neural language model word embeddings\footnote{\url{http://homepages.inf.ed.ac.uk/s1066731/dl.php?file=wordVectors.emnlp2012.zip&db=1}}.
The results are given in Table~\ref{table blacoe}. IORNN is the best on adj-n and v-obj, and second on noun-noun.
\begin{table}[h!]
	\center
	\begin{tabular}{ccccc}
		dim. & model & adj-n & n-n & v-obj \\ \hline 
		50 & add. & 0.28 & \textbf{0.26} & 0.24 \\ 
		50 & mult. & 0.26 & 0.22 & 0.18 \\ 
		100 & RAE & 0.20 & 0.18 & 0.14 \\ \hline
		50 & IORNN & \textbf{0.30} & 0.23 & \textbf{0.28} \\ \hline
	\end{tabular}
	\caption{Spearman correlation coefficients of model predictions for the phrase similarity task
	with neural language model word embeddings from \cite{blacoe_comparison_2012}.}
	\label{table blacoe}
\end{table}

\cite{hermann2013role} extend RAE with the help of Combinatory Categorial Grammar (CCG). 
Their models, named Combinatory Categorial Autoencoder (CCAE), are similar 
to RAE but use different parameter sets for different grammatical rules and grammatical types. 
Thank to this semantic-related grammar, their models outperform RAE and score towards 
the upper end of the range of addition and pair-wise multiplication. Table~\ref{table hermann}
shows the comparison between IORNN and other methods whose results are copied from 
the corresponding papers. 

\begin{table}[h!]
	\center
	\begin{tabular}{ccccc}
		\hline
		model & adj-n & n-n & v-obj \\ \hline \hline
		
		\multicolumn{4}{l}{Blacoe \& Lapata} & \\
		a./m. & 0.21 - 0.48 & 0.22 - 0.50 & 0.18 - 0.35 \\ 
		RAE & 0.19 - 0.31 & 0.24 - 0.30 & 0.09 - 0.28 \\ \hline 
		
		\multicolumn{4}{l}{Hermann \& Blunsom} & \\
		CCAEs & 0.38 - 0.41 & 0.41 - 0.44 & 0.23 - 0.34 \\ \hline 
			
		\multicolumn{4}{l}{Our implementation} & \\
		add. & 0.30 & 0.43 & 0.30 \\ 
		mult. & 0.14 & 0.24 & 0.16 \\ 
		IORNN & 0.38 & 0.36 & 0.32 \\ \hline
	\end{tabular}
	\caption{Spearman correlation coefficients of model predictions for the phrase similarity task.}
	\label{table hermann}
\end{table}

IORNN's performance lies in the range of CCAEs on adj-n and v-obj, 
but worse on noun-noun. However, it is worth emphasizing that IORNN uses one 
parameter set for all grammatical rules (similarly to RAE). 
From the difference of performance between RAE and CCAEs, we expect that extending 
IORNN in the same way (i.e., using CCG and different parameter sets for different 
grammatical rules and grammatical types) will lead to better performance.

%%%%%%%%%%%%%%%%%%%%

\subsection{Word Similarity in Context}
\label{subsection wsc} 

Differing from the first task, this task focuses on word meaning in context: 
it examines how well a model can make use of context to disambiguate word meaning. 
In this experiment, we use the Stanford Word Similarity in Context (SWSC) dataset
from \cite{huang2012improving} which contains 2003 word pairs, their sentential 
contexts, and human ratings from 0 to 10 (see Table~\ref{table swsc}).

\begin{table*}[!ht]
	\center
	\begin{tabular}{|p{6cm}|p{6cm}|p{3cm}|}
		\hline
		word 1 & word 2 & human ratings \\ \hline \hline
		
		 Located downtown along the east \textit{bank} of the Des Moines River, the plaza is available for parties, ... & This is the basis of all \textit{ money} laundering, ... &  0.0, 0.0, 3.0, 10.0, 8.0, 0.0, 4.0, 0.0, 0.0, 0.0 \\ \hline
	\end{tabular}
	\caption{An example in the SWSC dataset.}
	\label{table swsc}
\end{table*}

For IORNN, we represent the meaning of a word in its sentential context by concatenate its inner 
and outer meanings, i.e. $\mathbf{m}_w = [\mathbf{i}_w;\mathbf{o}_w]$. 
For vector addition, we compute the context meaning by averaging the meaning vectors of 5 words
on the left and 5 words on the right and then concatenate it with the meaning vector of the target 
word.

Similarly to the first experiment, we also use the cosine distance to measure the semantic similarity
and compute Spearman correlation coefficient $\rho$ between model scores and human judgements.

We compare IORNN and vector addition with the method proposed by \cite{huang2012improving}
(HSMN-M AvgSimC) and the one proposed by \cite{reisinger2010multi} (Pruned tf-idf-M AvgSimC).
These two methods are multi-prototype approaches: the meaning of a word is represented 
by multiple vectors (i.e., prototypes). In order to extract multipe prototypes for a word, they compute 
a vector for each context that the word is in, then cluster those context vectors. In this way, a prototype
corresponds to the centroid of a cluster, and hence represent a sense of the word. 
In order to compute the word meaning similarity with context, they use AvgSimC metric 
\begin{align*}
	&\text{AvgSimC}(w,w') = \\
	&\frac{1}{k^2} \sum_{i=1}^k \sum_{j=1}^k p(c,w,i)p(c',w',j)d(\mu_i(w),\mu_j(w'))
\end{align*}
where $k$ is the number of prototypes of each word, $p(c,w,i)$ is the likelihood that word $w$ is in 
its cluster $i$ given context $c$, $\mu_i(w)$ is the $i$-th cluster centroid of $w$ and $d(v,v')$ is a function 
computing similarity between two vectors.

Table~\ref{table swsc result} shows the comparison between the methods.
It is not surprising to see HSMN-M AvgSimC and Pruned tf-idf-M AvgSimC perform best since disambuguiating
word sense is the key to success and these methods use different vectors to represent different senses of a word.
However, IORNN, which represents the meaning of a word by a vector, performs comparably 
with Pruned tf-idf-M AvgSimC, and higher than the vector addition. It is worth noting the improvement from 
without using context (C\&W) to using context meaning computed by IORNN (from 58.0 to 60.2), thus 
confirming the abbility of IORNN in computing word meaning in context.

\begin{table}[!ht]
	\center
	\begin{tabular}{cc}
		\hline
		Model & $\rho \times 100$ \\ \hline \hline

		C\&W (w/o context) & 58.0 \\ \hline
		
		\multicolumn{2}{l}{Huang et al.} \\
		HSMN-M AvgSimC & 65.7 \\ 
		Pruned tf-idf-M AvgSimC & 60.5 \\ \hline 
		
		\multicolumn{2}{l}{Our implementation} \\
		add. & 59.0 \\
		IORNN & 60.3 \\ \hline
		
	\end{tabular}
	\caption{Spearman correlation coefficients of model predictions on the SWCS dataset.
	C\&W is the method to use the Collobert \& Wetson word embeddings without taking context into account. }
	\label{table swsc result}
\end{table}


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Summary} 

Now, we combine the experimental results presented above to compare IORNN 
with the two baselines, vector addition and vector pair-wise multiplication in 
Table~\ref{table summary}. In phrase similarity task, IORNN outperforms the both baselines
on adj-n and v-obj and worse than addition on noun-noun [why???]. In word similarity in context 
task, IORNN also outperforms the both baselines. These results show us that we can 
tackle the two problems, compositional semantics and meaning in context, with the unified framework 
IORNN.

\begin{table}[!ht]
	\center
	\begin{tabular}{c|ccc|c}
	Model & \multicolumn{3}{c}{Phrase similarity} & WSC \\ 
	& adj-n & n-n & v-obj &  \\ \hline
	
	add. & 0.30 & \textbf{0.43} & 0.30 & 59.0\\ 
	mult. & 0.14 & 0.24 & 0.16 & - \\ 
	IORNN & \textbf{0.38} & 0.36 & \textbf{0.32} & \textbf{60.3} \\ \hline
	
	\end{tabular}
	\caption{Comparison of IORNN against vector addition and pair-wise multiplication
	in the two tasks.}
	\label{table summary}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{section discussion}
In this section, we will discuss two important issues. The first one is about the cognitive 
plausibility of IORNN. The second is about potential extensions for it.

\subsection{Cognitive Plausibility}
\label{subsection cog plau}

We found that it is cognitively plausible to 
represent context meaning seperatively from phrase/word meaning, which we have called
\textit{outer meaning} and \textit{inner meaning} respectively. The key point here
is what is called \textit{dynamic binding} in connectionsim [cite...] where, in our case, 
outer meaning could be seen as slots and inner meaning as fillers (see Figure~\ref{figure inner outer}). 
Similar to hierarchical prediction network proposed by \cite{borensztajn2009hierarchical}, 
if the outer meaning and inner meaning are strongly correlated, a dynamic binding occurs
and connects the neurons at the tree root node of the phrase to the neurons at the 
corresponding node of the tree of the context. This explains why a human can use  
context meaning to predict the meaning of a unknown word, and why (s)he can select a word/phrase 
to fill in a blank in a uncomplete sentence.

\begin{figure}[h!]
	\center
	\includegraphics[scale=0.5]{Inner_outer.png}
	\caption{If there is a strong correlation between outer meaning and inner meaning, a dynamic
	binding occurs.}
	\label{figure inner outer}
\end{figure}



\subsection{Potential Extensions / Future Work}
\label{subsection future work}

Our new architecture IORNN has many potental extensions, some of which are promising to improve 
the performance in those two tasks presented above, other could lead to important applications. 

\paragraph{At lexicon level} In Subsection~\ref{subsection wsc}, we 
have seen multi-prototype approaches are potental for computing word meaning in 
context. Certainly, we can combine these approaches and our framework IORNN at the 
lexical level in order to reduce ambiguity introducing by lexical semantics. 

\paragraph{At syntax level} IORNN only uses parse trees without grammatical categories. However, 
Herman \& Blunsom emperically show the important role of syntax in vector space models of 
compositional semantics. It turns out that it is also easy to extend IORNN in the same way, 
i.e. using CCG and different parameter sets for different 
grammatical rules and grammatical types. Thanks to some degree of similarity between IORNN and 
RAE, we expect that this extension helps IORNN imporve its capacity in capturing compositionality.

\paragraph{At discourse level} In this paper, we propose IORNN as an architecture 
processing individual sentences; therefore, the outer meaning at the root node is alway a 
null-context outer meaning vector (i.e., $\mathbf{o}_{root} = \mathbf{o}_{\emptyset}$).
It turns out that it is easy to extend IORNN to make use of discourse context.
Figure~\ref{figure dciornn} illustrates how to connect inner and outer meanings 
of sentences in a discourse. Intuitively, the outer meaning of a sentence is 
the function of the inner meanings of its neighbour sentences. 

\begin{figure}[h!]
	\center
	\includegraphics[scale=0.5]{DC-IO-RNN.png}
	\caption{IORNN with Discourse Context. 
	Black rectangles correspond to inner meanings, 
	white rectangles correspond to outer meanings.}
	\label{figure dciornn}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{section conclusion}


\bibliographystyle{apalike}
\bibliography{ref}

\end{document}
